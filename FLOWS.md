# Flow Evolution Tracking

## Overview

This document tracks the evolution of Langflow orchestrations over time. Each flow variant is versioned, tested, and compared against baselines.

## Flow Versioning System

Format: `society_v{generation}_{variant}.json`

- **Generation**: Iteration number (1, 2, 3...)
- **Variant**: Letter suffix for A/B testing within same generation (a, b, c)

Examples:
- `society_v1_base.json` - Initial 4-agent design
- `society_v2_a.json` - Generation 2, variant A (added loop detection)
- `society_v2_b.json` - Generation 2, variant B (different voting mechanism)

---

## Flow Evolution Log

### Generation 0: Baseline (Pre-Society)

**File**: N/A (used single `langflow/a746f11d-02fa-4c41-8d48-01a49ad702a6`)

**Architecture**: Single LLM flow, no agent specialization

**Performance** (across 3 test games):
- Zork1: 5% progress, 10 points, 50 steps
- Enchanter: 0% progress, 0 points, 50 steps
- Hitchhiker: 0% progress, 0 points, 50 steps
- **Average**: 1.7% progress

**Failure Modes Observed**:
- Action loops (repeating north/south)
- Not building spatial model
- Forgetting items in inventory
- Trying same failed action multiple times
- No systematic exploration strategy

**Insights**:
- Single agent gets overwhelmed by multiple concerns
- No memory of past actions
- No spatial reasoning
- Needs specialization

**Decision**: Build society-of-mind with 4 specialists

---

### Generation 1: Society of Mind (MVP)

#### Flow v1_base.json

**Status**: üöß In Development

**Architecture**:
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Strategy Agent (Coordinator)        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ                                    ‚îÇ
     ‚ñº                                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇNavigator ‚îÇ  ‚îÇ  Puzzle  ‚îÇ  ‚îÇ  Memory  ‚îÇ
‚îÇ  Agent   ‚îÇ  ‚îÇ  Solver  ‚îÇ  ‚îÇ Tracker  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ            ‚îÇ              ‚îÇ
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ
        Shared Blackboard
        (JSON store)
```

**Components**:

1. **Shared Blackboard** (JSON state store)
   - Stores: map, inventory, action history, puzzle state
   - All agents read/write to shared state
   - Persistent across steps within single game

2. **Navigator Agent**
   - Input: Current observation, movement history
   - Output: Location name, suggested directions, map state
   - Prompt: "You are a navigation specialist..."

3. **Puzzle Solver Agent**
   - Input: Observation, inventory, known objects
   - Output: Suggested object interactions, puzzle hypotheses
   - Prompt: "You are a puzzle-solving specialist..."

4. **Memory Tracker Agent**
   - Input: Proposed action, full action history
   - Output: "Tried before?" flag, loop detection, recommendations
   - Prompt: "You are a memory specialist..."

5. **Strategy Agent (Coordinator)**
   - Input: Observation + all specialist responses
   - Output: Final action decision + reasoning
   - Prompt: "You coordinate specialist agents..."

**Decision Flow**:
1. Observation arrives ‚Üí Blackboard updated
2. Strategy queries relevant specialists in parallel
3. Specialists return responses (JSON format)
4. Strategy synthesizes ‚Üí final action
5. Action executed ‚Üí result added to blackboard ‚Üí repeat

**Communication Protocol**:
- All messages in JSON format
- Schema: `{"agent": "...", "answer": "...", "confidence": 0-1, "metadata": {...}}`
- Strategy agent weights by confidence

**Testing Plan**:
- Run on Zork1, Enchanter, Hitchhiker (same as baseline)
- Compare: progress %, score, steps to completion
- Log: agent contributions, decision rationale, state evolution

**Expected Improvements**:
- Better navigation (no loops, systematic exploration)
- Better memory (no action repetition)
- Better puzzle-solving (track inventory, try novel interactions)
- Target: >10% average progress (vs 1.7% baseline)

**Implementation Status**:
- [ ] Blackboard infrastructure
- [ ] Navigator agent prompt + logic
- [ ] Puzzle solver agent prompt + logic
- [ ] Memory tracker agent prompt + logic
- [ ] Strategy coordinator prompt + logic
- [ ] Langflow JSON generation
- [ ] Testing on 3 games

**Created**: 2025-11-24
**Creator**: Initial design
**Reason**: First society-of-mind implementation

---

### Generation 2: TBD (Reflection-Driven)

**Status**: üîÆ Future

**How this generation will be created**:
1. Run Generation 1 on 3 games
2. Collect logs and performance metrics
3. Run reflection system (rule-based + LLM analysis)
4. Identify failure patterns
5. Generate improvement suggestions
6. Mutate flow based on suggestions
7. Create v2 variants

**Potential improvements** (hypothetical examples):
- If looping persists ‚Üí Add explicit loop detector to Memory
- If puzzle-solving weak ‚Üí Add puzzle pattern library
- If coordination slow ‚Üí Change voting mechanism
- If agents conflict ‚Üí Add confidence calibration

**Performance target**: >20% average progress

---

## Flow Comparison Framework

### Metrics Tracked Per Flow

| Metric | Description | Target |
|--------|-------------|--------|
| **Progress %** | Average game completion across test suite | Higher is better |
| **Score** | Average points earned | Higher is better |
| **Steps to Progress** | Steps per 5% progress checkpoint | Lower is better |
| **Loop Count** | Number of action loops detected | 0 is ideal |
| **Novel Actions** | Unique actions per game | Higher is better |
| **Agent Query Count** | How often each specialist consulted | Balance is good |
| **Decision Confidence** | Average confidence of Strategy decisions | 0.7-0.9 ideal |
| **Execution Time** | Seconds per step (API latency) | Lower is better |

### A/B Testing Protocol

When testing multiple variants of same generation:

1. **Run in parallel** on same game set
2. **Randomize order** to avoid bias
3. **Same random seed** for reproducibility
4. **Run 3x each** to account for LLM variance
5. **Statistical significance** test (t-test on progress %)

Example:
```
Testing v2_a vs v2_b:
- Each plays Zork1, Enchanter, Hitchhiker
- 3 runs each (18 total games)
- Compare mean progress %
- Winner becomes v3_base
```

---

## Langflow JSON Schema Reference

### Key Components

A Langflow flow JSON contains:
```json
{
  "data": {
    "nodes": [
      {
        "id": "node-id-123",
        "type": "ChatInput | ChatOutput | Prompt | LLM | ...",
        "position": {"x": 100, "y": 200},
        "data": {
          "node": {
            "template": {
              "param_name": {"value": "..."}
            }
          }
        }
      }
    ],
    "edges": [
      {
        "source": "node-id-123",
        "target": "node-id-456",
        "sourceHandle": "output",
        "targetHandle": "input"
      }
    ]
  }
}
```

### Common Node Types

- **ChatInput**: Entry point (game observation)
- **ChatOutput**: Exit point (final action)
- **Prompt**: Text template for agent instructions
- **LLM**: Language model call (Claude, GPT, etc.)
- **JSONOutput**: Structured output parser
- **Conditional**: If/else branching
- **Code**: Custom Python code
- **Memory**: State storage (session memory)

### Society-of-Mind Flow Pattern

```
ChatInput (observation)
    ‚Üì
Blackboard Update (Python code node)
    ‚Üì
Strategy Agent (LLM + Prompt)
    ‚Üì
Parallel Specialists:
    ‚îú‚Üí Navigator (LLM + Prompt)
    ‚îú‚Üí Puzzle Solver (LLM + Prompt)
    ‚îî‚Üí Memory Tracker (LLM + Prompt)
    ‚Üì
Synthesize (Python code node)
    ‚Üì
ChatOutput (final action JSON)
```

---

## Programmatic Flow Generation

### Flow Generator API (To Be Built)

```python
from src.evolution.flow_generator import FlowGenerator

# Create base society flow
generator = FlowGenerator()

# Add agents
navigator = generator.add_agent(
    name="Navigator",
    prompt_template="navigator_prompt.txt",
    output_schema={"location": "str", "suggested_action": "str"}
)

puzzle = generator.add_agent(
    name="PuzzleSolver",
    prompt_template="puzzle_prompt.txt",
    output_schema={"actions": "list", "confidence": "float"}
)

memory = generator.add_agent(
    name="Memory",
    prompt_template="memory_prompt.txt",
    output_schema={"tried_before": "bool", "loop_detected": "bool"}
)

# Add coordinator
coordinator = generator.add_coordinator(
    specialists=[navigator, puzzle, memory],
    voting_mechanism="weighted_confidence"
)

# Generate flow JSON
flow_json = generator.generate()
generator.save("flows/society_v1_base.json")
```

### Flow Mutation API (To Be Built)

```python
from src.evolution.flow_mutator import FlowMutator

# Load existing flow
mutator = FlowMutator.load("flows/society_v1_base.json")

# Apply mutation based on reflection
mutator.add_subcomponent(
    parent="Memory",
    component="LoopDetector",
    prompt="Detect if last N actions repeat a pattern..."
)

# Change parameter
mutator.set_parameter(
    node="Strategy",
    param="confidence_threshold",
    value=0.7
)

# Save variant
mutator.save("flows/society_v2_a.json")
```

---

## Reflection-Driven Evolution Examples

### Example 1: Loop Detection Enhancement

**Observation** (from game logs):
```
Step 10-15: "go north", "go south", "go north", "go south", "go north"
Progress: 0% (stuck)
```

**Rule-Based Detector**: "Action loop detected: alternating north/south"

**LLM Reflection**:
```
"The Memory agent is not catching loops early enough. The system only detects
loops after 5 repetitions, but should catch after 3. Also, Memory should veto
actions that would continue a detected loop."
```

**Suggested Mutation**:
```python
mutator.set_parameter(
    node="Memory",
    param="loop_window_size",
    value=3  # Was 5
)

mutator.add_logic(
    node="Memory",
    logic="veto_if_continues_loop",
    priority="high"
)
```

**Result**: v2_a.json with enhanced loop detection

---

### Example 2: Navigator Needs Map Visualization

**Observation**:
```
Game: Zork1
Progress: 5% (barely entered house)
Analysis: Navigator mentioned 15 locations but doesn't track connections well.
Agents keep going to already-explored areas.
```

**LLM Reflection**:
```
"Navigator needs better spatial representation. Current state just lists
locations. Should build connection graph and track unexplored exits
systematically. Suggest adding a 'map builder' subcomponent."
```

**Suggested Mutation**:
```python
mutator.add_subcomponent(
    parent="Navigator",
    component="MapBuilder",
    prompt="""
    Build directed graph of location connections.
    Track: locations, exits, which exits explored, dead ends.
    Suggest: unexplored exits > backtracking > re-visiting.
    """
)
```

**Result**: v2_b.json with enhanced navigation

---

### Example 3: New Agent Creation

**Observation**:
```
Game: Hitchhiker's Guide
Progress: 0%
Failure: Missed critical early puzzle (getting out of bed).
Analysis: Complex NPC interaction with Ford, missed dialogue options.
```

**LLM Reflection**:
```
"This game has heavy NPC interaction and dialogue trees. None of the 4 current
agents specialize in NPCs. Puzzle Solver treats NPCs like objects, which fails.

RECOMMENDATION: Create new 'NPC Tracker' agent:
- Tracks which NPCs exist
- Remembers dialogue history per NPC
- Suggests social actions (talk, ask about X, give item to Y)
- Manages quest state"
```

**Suggested Mutation**:
```python
generator = FlowGenerator.load("flows/society_v2_base.json")

# Add 5th agent
npc_tracker = generator.add_agent(
    name="NPCTracker",
    prompt_template="npc_tracker_prompt.txt",
    output_schema={"npcs": "list", "suggested_dialogue": "str"}
)

# Update coordinator to query 5 agents
generator.update_coordinator(specialists=[navigator, puzzle, memory, npc_tracker])

generator.save("flows/society_v3_base.json")
```

**Result**: v3_base.json with 5-agent architecture

---

## Performance Tracking Dashboard

### Flow Comparison Table

| Flow | Zork1 Progress | Enchanter Progress | Hitchhiker Progress | Avg Progress | Avg Steps | Status |
|------|----------------|--------------------|--------------------|--------------|-----------|--------|
| v0 (baseline) | 5% | 0% | 0% | 1.7% | 50 | ‚úÖ Complete |
| v1_base | ? | ? | ? | ? | ? | üöß Testing |
| v2_a | - | - | - | - | - | üîÆ Future |
| v2_b | - | - | - | - | - | üîÆ Future |

*(Table updated after each test run)*

---

## Best Practices for Flow Evolution

### Do's ‚úÖ
- Version every flow variant
- Document reason for each change
- Run A/B tests before declaring winner
- Keep successful flows as reference
- Track metrics consistently
- Let reflection system drive changes

### Don'ts ‚ùå
- Don't hand-tune without data
- Don't change multiple things at once (can't identify what worked)
- Don't skip baseline comparison
- Don't discard "failed" variants (learn from them)
- Don't over-fit to single game

---

## Future Evolution Directions

Potential architectural innovations to explore:

1. **Hierarchical Agencies**
   - Meta-coordinators managing sub-societies
   - Example: "Exploration Agency" (Navigator + Memory) vs "Puzzle Agency" (Puzzle + Inventory)

2. **Dynamic Agent Spawning**
   - Create temporary specialists for specific puzzles
   - "NPC Tracker" only spawned when NPC detected

3. **Multi-Model Ensembles**
   - Different LLMs for different agents
   - Fast model for Memory, powerful model for Strategy

4. **Reinforcement Learning Integration**
   - Train action selector based on reward (progress %)
   - Learn optimal agent weighting

5. **Cross-Game Transfer Learning**
   - Strategy library: patterns that work across games
   - Meta-learning: "this type of game ‚Üí use this flow variant"

---

## Implementation Status

- [x] Documentation structure created
- [ ] v1_base.json: Society-of-mind MVP
- [ ] Flow generator API
- [ ] Flow mutator API
- [ ] A/B testing framework
- [ ] Performance tracking dashboard
- [ ] Reflection ‚Üí mutation pipeline

---

## Next Steps

1. Build Langflow JSON generator (`src/evolution/flow_generator.py`)
2. Create v1_base.json programmatically
3. Test v1_base on 3 games and collect metrics
4. Run reflection system on results
5. Generate v2 variants based on feedback
6. Iterate!

---

**Last Updated**: 2025-11-24
**Current Generation**: 1 (in development)
**Total Flows Created**: 0
**Best Performing Flow**: TBD
